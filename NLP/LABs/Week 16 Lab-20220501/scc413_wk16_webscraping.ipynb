{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DjX8Gcz4qk2"
      },
      "source": [
        "# SCC.413 Applied Data Mining\n",
        "# NLP: Week 16\n",
        "# Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKAHHD354qk4"
      },
      "source": [
        "## Contents\n",
        "- [Introduction](#intro)\n",
        "- [Packages & imports](#packages)\n",
        "- [Justext](#justext)\n",
        "- [Requests](#requests)\n",
        "- [Beautiful Soup](#beautiful_soup)\n",
        "- [Wikipedia exercise](#wiki_assessed)\n",
        "- [Optional Wikipedia exercise](#wiki_optional)\n",
        "- [Forum exercise](#forum)\n",
        "- [Advanced Forum exercise](#forum_advanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcPGQ5Hl4qk5"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Introduction\n",
        "\n",
        "If you use spidering or just download a list of webpage URLs (e.g. with [curl](https://curl.haxx.se/docs/manpage.html) or [requests](https://requests.readthedocs.io/)), you will be left with a collection of HTML files. These raw HTML files contain a lot of redundant information (e.g. adverts, menus, headings, etc.), this is known as \"boilerplate\". What we want is the main text of the webpage only (i.e. the news article, the blog post, etc.), in plain text without the HTML tags. If you are not familiar with HTML, [there is a guide here](https://www.w3schools.com/html/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUHOdYSW4qk6"
      },
      "source": [
        "<a name=\"packages\"></a>\n",
        "## Packages & Imports\n",
        "\n",
        "The packages required for the week's lab work are already available on Colab, except for justext. Running the cell below will install justext on Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install justext"
      ],
      "metadata": {
        "id": "jktsYNHX6Plq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-standard packages are also included in `requirements.txt`, if you need to install them on your own machine.\n",
        "\n",
        "The imports for all of the code in this lab are provided in one cell here for convenience."
      ],
      "metadata": {
        "id": "AnuL4Z_a6QGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AkeN7Vn4qk7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import justext\n",
        "import requests\n",
        "from urllib.parse import urljoin #https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin\n",
        "from bs4 import BeautifulSoup, Tag #https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSo9cMTl4qk-"
      },
      "source": [
        "<a name=\"justext\"></a>\n",
        "## Justext\n",
        "\n",
        "Webpages come in all shapes and sizes, and it varies massively how easy it is to scrape the text of interest. If we are lucky, it is relatively easy to pick out the text, and there are fully automated tools available for that. One of these tools is [justext](https://github.com/miso-belica/jusText).\n",
        "\n",
        "1. Have a read through the [description of the justext algorithm](http://corpus.tools/wiki/Justext/Algorithm).\n",
        "2. There is a good [online demo of the tool](https://nlp.fi.muni.cz/projects/justext/). Try out a webpage, e.g. a BBC news article, this article demonstrates it well: <http://www.bbc.co.uk/bbcthree/article/cc72247b-e658-4af8-a838-dfe4e68e2776>.\n",
        "3. Manually compare the filtered text and the text on the web page. How accurate is the text extraction? How much is missed, how much text is incorrectly included. What are the potential impacts of this in a later analysis of the text?\n",
        "4. You can use justext via Python too. The HTML needs to be downloaded from a website, or using a file already collected. The `print_justext` function below takes some html and prints out the justext output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_justext(html):\n",
        "  paragraphs = justext.justext(html, justext.get_stoplist(\"English\"))\n",
        "  for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "      tag = 'h' if paragraph.heading else 'p'\n",
        "      print(\"<{}> {}\".format(tag, paragraph.text))"
      ],
      "metadata": {
        "id": "n5AdM1EKNtVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. You can use this function to process a previously gathered webpage, or a sample webpage is available alongside this notebook (/content/5.html), as shown below.\n",
        "6. Examine the text produced. How much of the text is correctly outputted?\n",
        "7. Note the `<h>` and `<p>` tags, indicating headers and paragraphs. These may be useful, if for instance you are only interested in headers or the actual body of the text, or want to separate them in later analysis. You'll learn next week how you could filter these, e.g. using regular expressions."
      ],
      "metadata": {
        "id": "lKPMI8cN52ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/5.html\", \"r\") as html_file:\n",
        "  html = html_file.read()\n",
        "  print_justext(html)"
      ],
      "metadata": {
        "id": "tgIywfO978dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwvAyOXo4qk_"
      },
      "source": [
        "Often an automated approach will not provide accurate enough results. Fortunately, there are other methods available, other than just manually copying and pasting the text. Many tools have been built that assist in parsing web pages and extracting the text of interest, although scripts need writing using these tools for different sets of websites. Some mimic a user's interaction with a website to get to the relevant data (e.g. [Selenium](http://seleniumhq.github.io/selenium/docs/api/py/)). [Scrapy](https://doc.scrapy.org/en/latest/index.html) is another good option for grabbing and processing webpages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYQCg6T44qlA"
      },
      "source": [
        "<a name=\"requests\"></a>\n",
        "## Requests\n",
        "\n",
        "Here we will be using the Python requests package, which makes downloading webpages easy: <https://requests.readthedocs.io/>.\n",
        "\n",
        "We simply provide a URL. This can be fed into justext, as below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(\"http://www.bbc.co.uk/bbcthree/article/cc72247b-e658-4af8-a838-dfe4e68e2776\")\n",
        "print_justext(response.text)"
      ],
      "metadata": {
        "id": "NQiQkYHI6s_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkegxThF4qlB"
      },
      "source": [
        "<a name=\"beautiful_soup\"></a>\n",
        "## Beautiful Soup\n",
        "\n",
        "Requests will provide raw HTML files. The key part of web scraping is extracting the relevant parts of the webpage. For this we will use Beautiful Soup: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oR6bvEJ4qlB"
      },
      "source": [
        "The basic process is to look at the HTML of the target webpage and look for ways of drilling down to the elements of interest, with the overall aim of extracting just the specific text of interest. This could be metadata, or actual text for further analysis. There are numerous methods provided by Beautiful Soup, please consult [the documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for options available.\n",
        "\n",
        "All websites are different (although some have consistent structures), and often a custom scraper needs to be developed. You can use a standard web browser to look at the information of interest, right-click on the first part of the data of interest and select \"Inspect Element\" (Firefox & Safari) or \"Inspect\" (Chrome), you will then see the HTML code for that element, and the surrounding elements.\n",
        "\n",
        "You can then use Beautiful Soup's functions for drilling down and traversing the relevant parts of the web page. You can also extract links and use the requests package to download further webpages for processing.\n",
        "\n",
        "To demonstrate, we will be parsing Wikipedia pages. As an example we will look to extract plot summaries for Star Trek episodes: <https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes>. We download the webpage using requests, and then use Beautiful Soup to put the html into a parseable document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TJH6ZvH4qlC"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes\"\n",
        "#load webpage\n",
        "req = requests.get(base_url)\n",
        "soup = BeautifulSoup(req.text, \"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7p-UqZi4qlD"
      },
      "source": [
        "Use a web browser to view the [Wikipedia page](https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes). We are targetting the list of episodes in the tables starting under \"Season 1 (1966–67)\". Right click on the table cell with the first episode title (\"The Man Trap\") and \"Inspect Element\" (or just \"Inspect\" in Chrome). Note that this cell (`td`) is of class \"summary\", as is every title cell, and other cells are not. This is our way in. We are going to collect a list of titles from the table, along with the URL of the Wikipedia page about that episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H6i4J2-4qlE"
      },
      "outputs": [],
      "source": [
        "episodes = [] #to store the list of episodes.\n",
        "#find and loop through all tds (table cells) with class name ``summary'' (which we know is an episode title)\n",
        "for episode_cell in soup.find_all('td', {'class': 'summary'}):\n",
        "    title = episode_cell.a.text.strip() #Get the actual text from the cell.\n",
        "    episode_url = episode_cell.a['href'] #extract the url\n",
        "    episodes.append({'title': title, 'url': episode_url}) #store in dictionary format\n",
        "    \n",
        "episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGGR6kCA4qlE"
      },
      "source": [
        "Note the URLs are relative, they can be made absolute with urljoin, using the base url as a reference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duD_QbcL4qlF"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urljoin #https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin\n",
        "for episode in episodes:\n",
        "    episode['url'] = urljoin(base_url, episode['url'])\n",
        "    \n",
        "episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7REDjZO4qlG"
      },
      "source": [
        "Now we have a list of episodes and their individual wikipedia pages for downloading. We can try justext on the first episode to see if automatic extraction will do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nAiN0j64qlG"
      },
      "outputs": [],
      "source": [
        "episode_req = requests.get(episodes[0]['url']) #use requests to download episode webpage.\n",
        "\n",
        "paragraphs = justext.justext(episode_req.text, justext.get_stoplist(\"English\"))\n",
        "text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    if not p.is_boilerplate:\n",
        "        text += p.text.strip() + \"\\n\"\n",
        "        \n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF5GgfMb4qlH"
      },
      "source": [
        "Compare this to the [Wikipedia page](https://en.wikipedia.org/wiki/The_Man_Trap). It does a pretty good job of getting the text from the whole page, however, we want to only include the plot section. To do this, we need to be more specific about what to extract, so we can use Beautiful Soup.\n",
        "\n",
        "Looking at the [Wikipedia page](https://en.wikipedia.org/wiki/The_Man_Trap), you can see the sections are headed with an `h2` element, so to find the \"Plot\" section we just need to go through the `h2` tags, find the one with \"Plot\" and hoover up all of the text between there and the next section. We can do this for the first episode as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4jwz8zn4qlH"
      },
      "outputs": [],
      "source": [
        "from bs4 import Tag #we need the Tag class from Beautiful Soup to check if a node we are looking at is Tag.\n",
        "\n",
        "episode_soup = BeautifulSoup(episode_req.text, \"html.parser\") #use beautiful soup to decode into a parseable document.\n",
        "\n",
        "episode_plot = \"\"\n",
        "\n",
        "for h2 in episode_soup.find_all(\"h2\"): #Go through all of the h2 elements.\n",
        "            if(h2.text.strip().startswith(\"Plot\")): #This is the h2 With \"Plot\" (and \"Plot Summary\")\n",
        "                node = h2.next_sibling #start looking for tags after the Plot h2, will be strings and Tags.\n",
        "                while True:\n",
        "                    if isinstance(node, Tag): #Check if this element is actually a Tag.\n",
        "                        if node.name == \"p\": #p tag, we want this.\n",
        "                            episode_plot += node.text.strip() + \"\\n\" #append the text from p.\n",
        "                        elif node.name == \"h2\": #at the next h2, so a new section, no longer the plot. Stop processing.\n",
        "                            break\n",
        "                    node = node.next_sibling #get next element at same level.\n",
        "                    \n",
        "print(episode_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJs4iC154qlI"
      },
      "source": [
        "You'll see we now just have the plot text. We just need to wrap this up in a loop and add the plot to each episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Gp1z3F8i4qlI"
      },
      "outputs": [],
      "source": [
        "for episode in episodes:\n",
        "    episode_plot = \"\"\n",
        "    episode_req = requests.get(episode['url']) #do a new request for the episode page.\n",
        "    episode_soup = BeautifulSoup(episode_req.text, \"html.parser\") #use beautiful soup to decode into a parseable document.\n",
        "    for h2 in episode_soup.find_all(\"h2\"): #Go through all of the h2 elements.\n",
        "        if(h2.text.strip().startswith(\"Plot\")): #This is the h2 With \"Plot\" (and \"Plot Summary\")\n",
        "            node = h2.next_sibling #start looking for tags after the Plot h2, will be strings and Tags.\n",
        "            while True:\n",
        "                if isinstance(node, Tag): #Check if this element is actually a Tag.\n",
        "                    if node.name == \"p\": #p tag, we want this.\n",
        "                        episode_plot += node.text.strip() + \"\\n\" #append the text from p.\n",
        "                    elif node.name == \"h2\": #at the next h2, so a new section, no longer the plot. Stop processing.\n",
        "                        break\n",
        "                node = node.next_sibling #get next element at same level.\n",
        "\n",
        "        episode['plot'] = episode_plot #add the plot to episode.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgyhF-ce4qlJ"
      },
      "source": [
        "As this is in dictionary format, it's nice to convert to JSON:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfOd_0qT4qlK"
      },
      "outputs": [],
      "source": [
        "print(json.dumps(episodes,indent=4)) #print out the resulting json (pretty printed)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the output can be saved to a file:"
      ],
      "metadata": {
        "id": "xAyRzy6zXZ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('startrek.json', 'w') as f:\n",
        "  #Dump json file. indent=4 prints the output prettier, but will increase disk space.\n",
        "  json.dump(episodes, f, indent=4)"
      ],
      "metadata": {
        "id": "Y5CwJ-2kXZkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH0a-0Ru4qlK"
      },
      "source": [
        "The code is split up here to explain it more neatly in a notebook, the the full code is available in `startrekscrape.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxDRFrC04qlL"
      },
      "source": [
        "<a name=\"wiki_assessed\"></a>\n",
        "\n",
        "## Exercise 1: Wikipedia scraping\n",
        "\n",
        "To practice using Beautiful Soup, try extracting details of films by Stanley Kubrick from this page: <https://en.wikipedia.org/wiki/Filmography_and_awards_of_Stanley_Kubrick>. The aim is to extract the year and title for the 13 feature films listed. To make it a little trickier, try only outputting films with Kubrick as a Producer. Some starting code is provided below.\n",
        "\n",
        "Hint: You can use [findAll with a limit](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-limit-argument) to return only a limited number of nodes from another node, e.g. `td`s in a `tr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUCpdDN64qlL"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Wikipedia scraping\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/Filmography_and_awards_of_Stanley_Kubrick\"\n",
        "#load webpage\n",
        "req = requests.get(base_url)\n",
        "#Use beautiful soup to decode webpage text into parseable document.\n",
        "soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "table = soup.find('table', {'class': 'wikitable'}) #just find 1 table, as it's the first 1\n",
        "trs = table.findAll('tr') #all table rows\n",
        "trs = trs[1:] #skip header row\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp5FG6bx4qlM"
      },
      "source": [
        "<a name=\"wiki_optional\"></a>\n",
        "### Optional Wikipedia Exercise\n",
        "\n",
        "You can also use what you've learnt to parse details of something else from Wikipedia, e.g. other TV series, or albums from an artist's discography."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7TVhzyT4qlN"
      },
      "outputs": [],
      "source": [
        "# Optional Task Wikipedia\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZgsdOzU4qlN"
      },
      "source": [
        "<a name=\"forum\"></a>\n",
        "## Exercise 2: Forum scraping\n",
        "\n",
        "The optional lab workbook uses Scrapy to download pages from a forum, you can perform a similar task for forums with Requests and Beautiful Soup. You can choose any forum and use what you've learnt to parse thread posts into plain text. Start with an individual thread, and one page of posts within that thread. A good example to try is from Mumsnet on noisy baby toys: <https://www.mumsnet.com/Talk/toys_and_games_chat/3414974-noisy-baby-toys-which-are-the-worst>.\n",
        "\n",
        "1. Try using justext to parse the the first page of posts, examine the results. Is this good enough?\n",
        "2. Use Beautiful Soup to collect just the posts from the first page and output as plain text.\n",
        "\n",
        "Hint: one issue you may come across is `<br>` tags instead of newlines. If ignored, this will run lines of text in the same post directly next to each other (without even a space), e.g. something horrible like this:\n",
        "\"Allllllll of the toot tootSpin and bounce zebraAnd best (worst!) of all... peppa pig alphaphonic board- it has no off switch and the slightest touch sets it off for ages...\"\n",
        "\n",
        "This makes tokenisation difficult and is tricky to rectify. This is a good lesson in sanity checking the exported text, better to have the text as close to as appears on the webpage now. To deal with this issue, the `<br>` tags can be replaced with a new line character (or some other marker), e.g.:\n",
        "\n",
        "```\n",
        "for br in post.find_all(\"br\"):\n",
        "    br.replace_with(\"\\n\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sONVbdw_4qlO"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: Forum scraping\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIH45tuU4qlO"
      },
      "source": [
        "<a name=\"forum_advanced\"></a>\n",
        "## Advanced Exercise: Forum paging\n",
        "\n",
        "You will notice that the topic's posts are spread across multiple pages, this means that \"paging\" needs to be performed to extract the posts from every page. This is a little tricky, but work from collecting 1 page. You will need to consult the [requests documentation](https://requests.readthedocs.io/) to discover how to pass parameters to match the links to other pages. Be careful not to have duplicate posts in your extraction (the original post is repeated on each page).\n",
        "\n",
        "The trickiest part is to know when to stop. Going past the number of pages available just brings the user to the final page. We will be covering regular expressions next week, so to help you out, the following code can be used to find the final page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmsCjaYp4qlP"
      },
      "outputs": [],
      "source": [
        "#this isn't complete code, you'll need to incorporate it into your solution.\n",
        "\n",
        "lastpage = re.compile(r\"page (\\d+) of \\1\") #compiled regular expression, checking if we're at page n of n, i.e. last page.\n",
        "\n",
        "#...\n",
        "\n",
        "pages = soup.find('div', {'class': 'pages'}).text.strip() #find pages element which has the \"This is page x of y\" text.\n",
        "if lastpage.search(pages) != None: #if our regex matches, then we're on last page, so make this the last one to parse.\n",
        "    at_last = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUXXDqHp4qlP"
      },
      "outputs": [],
      "source": [
        "# Advanced Exercise: Forum paging\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6_cCsKh4qlQ"
      },
      "source": [
        "You can go further by parsing multiple threads from a (page of a) subforum (or \"Talk\" on Mumsnet), e.g. try extracting all of the threads from the first page from baby names discussions: <https://www.mumsnet.com/Talk/baby_names>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBWzMZLj4qlQ"
      },
      "outputs": [],
      "source": [
        "# Advanced Exercise: Multiple threads\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "scc413_wk16_webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}