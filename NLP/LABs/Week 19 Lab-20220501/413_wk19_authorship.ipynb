{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm-InQZJmItZ"
      },
      "source": [
        "# SCC.413 Applied Data Mining\n",
        "# Week 18\n",
        "# Authorship attribution (with single Tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6KnDsWmItf"
      },
      "source": [
        "## Contents\n",
        "* [Introduction](#intro)\n",
        "* [Preamble](#preamble)\n",
        "* [Document object and processors](#doc)\n",
        "* [Dataset](#data)\n",
        "* [Pipeline](#pipeline)\n",
        "* [Error analysis](#error)\n",
        "* [Exercise](#ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8j-1XEDmItg"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Introduction\n",
        "\n",
        "In previous labs you have classified collections of Tweets (for a single user) as one document/instance in the classifier. Here, we instead treat individual tweets as documents, and attempt to classify these. We use authorship attribution as the task here, i.e. predicting the user who produced the Tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4UYzWBRmIth"
      },
      "source": [
        "<a name=\"preamble\"></a>\n",
        "## Preamble\n",
        "Below are imports and helper functions from previous labs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "id": "euN-k-Ja7dEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atoq9CDLmIti"
      },
      "outputs": [],
      "source": [
        "import ftfy\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import Binarizer, StandardScaler\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from os import listdir, makedirs\n",
        "from os.path import isfile, join, splitext, split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in previous notebooks, you should upload all of the provided files to a Google Drive folder, you can then access these files from your Python code. See also the files tab."
      ],
      "metadata": {
        "id": "tS7ckSyCBCoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "ydCluoej2UY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the folder we are working from as a variable for easy access. You may need to edit the path to match your own."
      ],
      "metadata": {
        "id": "trooi5ZMzSM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_folder = '/content/gdrive/MyDrive/413/wk19/'"
      ],
      "metadata": {
        "id": "c1YsiHLvzKeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code adds the working folder to the system path, so you can import Python files from this folder."
      ],
      "metadata": {
        "id": "ktoDdLMkBVMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(working_folder)"
      ],
      "metadata": {
        "id": "Cq_7TvOW34v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao04NlRkmItl"
      },
      "source": [
        "A couple of methods for showing classifier results (from 1st classification lab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqu7RrekmItm"
      },
      "outputs": [],
      "source": [
        "def print_cv_scores_summary(name, scores):\n",
        "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))\n",
        "    \n",
        "def confusion_matrix_heatmap(cm, index):\n",
        "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
        "    dims = (10, 10)\n",
        "    fig, ax = plt.subplots(figsize=dims)\n",
        "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
        "    ax.set_ylabel('Actual')    \n",
        "    ax.set_xlabel('Predicted')\n",
        "    \n",
        "def confusion_matrix_percent_heatmap(cm, index):\n",
        "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
        "    percents = cmdf.div(cmdf.sum(axis=1), axis=0)*100\n",
        "    dims = (10, 10)\n",
        "    fig, ax = plt.subplots(figsize=dims)\n",
        "    sns.heatmap(percents, annot=True, cmap=\"coolwarm\", center=0, vmin=0, vmax=100)\n",
        "    ax.set_ylabel('Actual')    \n",
        "    ax.set_xlabel('Predicted')\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.set_ticks([0, 25, 50, 75, 100])\n",
        "    cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUHWefJmItn"
      },
      "source": [
        "<a name=\"doc\"></a>\n",
        "## Document object and processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACx5AwXOmItn"
      },
      "source": [
        "Functions for processing text, and producing a Document class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-SObFiamIto"
      },
      "outputs": [],
      "source": [
        "hashtag_re = re.compile(r\"#\\w+\")\n",
        "mention_re = re.compile(r\"@\\w+\")\n",
        "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
        "\n",
        "def preprocess(text):\n",
        "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
        "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
        "    p_text = url_re.sub(\"[url]\",p_text)\n",
        "    p_text = ftfy.fix_text(p_text)\n",
        "    return p_text\n",
        "\n",
        "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
        "def tokenise(text):\n",
        "    return tokenise_re.findall(text)\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, meta={}):\n",
        "        self.meta = meta\n",
        "        self.tokens_fql = Counter() #empty Counter, ready to be added to with Counter.update.\n",
        "        self.ht_fql = Counter()\n",
        "        self.num_tokens = 0\n",
        "        self.text = \"\"\n",
        "        \n",
        "    def extract_features_from_text(self, text):\n",
        "        hts = hashtag_re.findall(text)\n",
        "        self.ht_fql.update([ht.lower() for ht in hts])\n",
        "        p_text = preprocess(text)\n",
        "        tokens = tokenise(p_text)\n",
        "        lower_tokens = [t.lower() for t in tokens]\n",
        "        self.num_tokens += len(lower_tokens)\n",
        "        self.tokens_fql.update(lower_tokens) #updating Counter counts items in list, adding to existing Counter items.\n",
        "        self.text += text\n",
        "        \n",
        "    def extract_features_from_texts(self, texts): #texts should be iterable text lines, e.g. read in from file.\n",
        "        for text in texts:\n",
        "            extract_features_from_text(text)\n",
        "            \n",
        "    def average_token_length(self):\n",
        "        sum_lengths = 0\n",
        "        for key, value in self.tokens_fql.items():\n",
        "            sum_lengths += len(key) * value\n",
        "        return sum_lengths / self.num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze5FN4KQmItp"
      },
      "source": [
        "A transformer to convert `Document` to extract features via a callable method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_RH5p4zmItq"
      },
      "outputs": [],
      "source": [
        "class DocumentProcessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, process_method):\n",
        "        self.process_method = process_method\n",
        "    \n",
        "    def fit(self, X, y=None): #no fitting necessary, although could use this to build a vocabulary for all documents, and then limit to set (e.g. top 1000).\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        for document in documents:\n",
        "            yield self.process_method(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Al66BImItr"
      },
      "outputs": [],
      "source": [
        "def get_tokens_fql(document):\n",
        "    return document.tokens_fql\n",
        "\n",
        "def get_ht_fql(document):\n",
        "    return document.ht_fql\n",
        "\n",
        "def get_text_stats(document):\n",
        "    ttr = len(document.tokens_fql) / document.num_tokens\n",
        "    return {'avg_token_length': document.average_token_length(), 'ttr': ttr }\n",
        "\n",
        "def read_list(file):\n",
        "    with open(file) as f:\n",
        "        items = []\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            items.append(line.strip())\n",
        "    return items\n",
        "\n",
        "fws = read_list(working_folder + \"functionwords.txt\")\n",
        "\n",
        "def get_fws_fql(document):\n",
        "    fws_fql = Counter({t: document.tokens_fql[t] for t in fws}) #dict comprehension, t: fql[t] is token: freq.\n",
        "    return +fws_fql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP5VIo84mIts"
      },
      "source": [
        "<a name=\"data\"></a>\n",
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5uGbBbgmItt"
      },
      "source": [
        "Import Tweets as single Document, with metadata of user included. You could utilise other metadata to predict party of user (or age/gender from celebs data) of a single Tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLPVKBFmmItt"
      },
      "outputs": [],
      "source": [
        "def import_tweets_json(folder):\n",
        "    jsonfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".json\")]\n",
        "    for jf in jsonfiles:\n",
        "        with open(jf) as f:\n",
        "            data = json.load(f)\n",
        "            tweets = data.pop('tweets')\n",
        "            metadata = data\n",
        "        print(\"Processing \" + metadata['screen_name'])\n",
        "        for tweet in tweets:\n",
        "            doc = Document(meta=metadata)\n",
        "            doc.extract_features_from_text(tweet['text'])\n",
        "            yield doc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using a small collection of previously collected mp twitter users. You could use users from the celebs dataset instead. Just be aware that for some datasets the `import_tweets_json` method will need updating to take `full_text` from the tweet, not `text`."
      ],
      "metadata": {
        "id": "_y2w25djAy9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1ZdKb60mItu"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus.extend(import_tweets_json(working_folder + \"mps-json-10\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLPwj6q8mItw"
      },
      "outputs": [],
      "source": [
        "y = [d.meta['screen_name'] for d in corpus]\n",
        "X = corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQvuZZhAmItx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
        "print(len(X_train), len(X_test))\n",
        "print(len(y_train), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wisPtpM4mIty"
      },
      "source": [
        "<a name=\"pipeline\"></a>\n",
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPkn4Wj-mItz"
      },
      "source": [
        "Here is a sample pipeline to be used with gridsearch, with a feature union of hashtags, words or function words, and some text stats. Using either a naive bayes or logisitic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbuozXeGmIt0"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list = [\n",
        "            ('hts', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = get_ht_fql)),\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "                ('binarize', Binarizer()),\n",
        "            ])),\n",
        "            ('word', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = None)), # to be set by grid search.\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "                ('binarize', Binarizer()),\n",
        "            ])),\n",
        "            ('stats', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = get_text_stats)),\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "        ],\n",
        "    )),\n",
        "    ('selector', SelectKBest(score_func = chi2)),\n",
        "    ('standardize', StandardScaler(with_mean=False)),\n",
        "    ('clf', None), # to be set by grid search.\n",
        "])\n",
        "\n",
        "param_grid={\n",
        "    'union__word__processor__process_method': [get_fws_fql, get_tokens_fql],\n",
        "    'selector__k': [50, 100, 150, 500],\n",
        "    'clf': [MultinomialNB(), LogisticRegression(solver='liblinear', random_state=0, multi_class='ovr')],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw84BZ6smIt1"
      },
      "outputs": [],
      "source": [
        "search = GridSearchCV(model, cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
        "                      return_train_score = False, \n",
        "                      scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
        "                      refit = 'f1_weighted',\n",
        "                      param_grid = param_grid\n",
        "                     )\n",
        "\n",
        "search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuroLkWpmIt1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNG2Jv4GmIt2"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "uq3raV5dmIt3"
      },
      "outputs": [],
      "source": [
        "predictions = search.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "\n",
        "confusion_matrix_percent_heatmap(confusion_matrix(y_test,predictions), search.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqghDvgymIt4"
      },
      "source": [
        "Once evaluated, we can see that we can predict the author of a Tweet quite accurately, with some users easier to predict than others. Why might this be?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxKRbIiPmIt5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q25eawSxmIt5"
      },
      "source": [
        "<a name=\"error\"></a>\n",
        "## Error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gm91tGDmIt6"
      },
      "source": [
        "We can perform some error analysis by looking at the text alongside the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91R0PqFUmIt6"
      },
      "outputs": [],
      "source": [
        "X_test_texts = [x.text for x in X_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FshUMTumIt7"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(zip(X_test_texts,y_test,predictions)), columns=[\"Tweet\", \"Actual\", \"Predicted\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtZ44rO6mIt8"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_colwidth = 300\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Z1R-vamIt_"
      },
      "source": [
        "For example, we can see when Theresa May's tweets are predicted incorrectly: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEajAxrFmIuA"
      },
      "outputs": [],
      "source": [
        "df[df['Actual'].str.match(\"@theresa_may\") & ~df['Predicted'].str.match(\"@theresa_may\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOA3BOC4mIuC"
      },
      "source": [
        "Or when predicted as a specific other person:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYnvf_Z2mIuD"
      },
      "outputs": [],
      "source": [
        "df[df['Actual'].str.match(\"@theresa_may\") & df['Predicted'].str.match(\"@jeremycorbyn\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCQinILfmIuD"
      },
      "source": [
        "<a name=\"ex\"></a>\n",
        "## Exercise\n",
        "\n",
        "The classifier above works quite well, but is using hashtags and words, which are going to be topic/time related. Would the same features work for tweets months or years apart? Try to develop a new feature set that is restricted to style features only, and evaluate on this. You could also use the celebs data, or use a larger set of authors, to test your new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYegg3T-mIuE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "413-wk19-authorship.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}