{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff6YhH2QjfY"
      },
      "source": [
        "# SCC.413 Applied Data Mining\n",
        "# Week 18\n",
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv1cIjIPQjfe"
      },
      "source": [
        "## Contents\n",
        "* [Introduction](#intro)\n",
        "* [Preamble](#preamble)\n",
        "* [Bag of Words](#bow)\n",
        "    - [Filtered List](#filtered)\n",
        "    - [Word N-grams](#wordn)\n",
        "* [Characters](#chars)\n",
        "    - [Char N-grams](#charn)\n",
        "* [Annotation](#ann)\n",
        "* [Other features](#other)\n",
        "* [Documents](#docs)\n",
        "* [Corpus analysis](#corpus)\n",
        "* [TF-IDF](#tfidf)\n",
        "* [Exercise](#ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaZjHUWQjfg"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Introduction\n",
        "\n",
        "In previous weeks we have collected data, preprocessed and cleaned it, and tokenised the text into meaningful units (\"words\"). Now with usable text and a token list, in this lab we will look to extract features by counting occurrences of different elements, and calculating other features over the text, tokens, and other features.\n",
        "\n",
        "A range of features will be looked at here, that can be used for a variety of analyses, however there are many other feature that can be extracted (see lecture slides and reading list). You should keep under consideration how preprocessing and tokenisation (your pipeline) can impact the features extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKcxCEnWQjfg"
      },
      "source": [
        "<a name=\"preamble\"></a>\n",
        "## Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should upload all of the provided files to a Google Drive folder, you can then access these files from your Python code. See also the files tab."
      ],
      "metadata": {
        "id": "tS7ckSyCBCoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "ydCluoej2UY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the folder we are working from as a variable for easy access. You may need to edit the path to match your own."
      ],
      "metadata": {
        "id": "trooi5ZMzSM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_folder = '/content/gdrive/MyDrive/413/wk18/'"
      ],
      "metadata": {
        "id": "c1YsiHLvzKeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code adds the working folder to the system path, so you can import Python files from this folder."
      ],
      "metadata": {
        "id": "ktoDdLMkBVMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(working_folder)"
      ],
      "metadata": {
        "id": "Cq_7TvOW34v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use code from last week to preprocess our text, a method is defined below to do some basic preprocessing, please check your understanding. You may see fit to edit the preprocessing to suit your needs later."
      ],
      "metadata": {
        "id": "yk7Hgn4cTIiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "id": "OeWhdio5RQ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFnSpQtjQjfh"
      },
      "outputs": [],
      "source": [
        "import ftfy\n",
        "import re\n",
        "\n",
        "hashtag_re = re.compile(r\"#\\w+\")\n",
        "mention_re = re.compile(r\"@\\w+\")\n",
        "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
        "\n",
        "def preprocess(text):\n",
        "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
        "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
        "    p_text = url_re.sub(\"[url]\",p_text)\n",
        "    p_text = ftfy.fix_text(p_text)\n",
        "    return p_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30lc6YknQjfk"
      },
      "source": [
        "To demonstrate the feature extraction, we're going to start by working with a single tweet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARWHw52oQjfl"
      },
      "outputs": [],
      "source": [
        "tweet = \"This week we‚Äôre at a #careers event in #Blackpool @Pleasure_Beach, talking to students about #languages and language careers! Come have a go at some of our activities! üåè#LoveLanguages #LoveLancaster @Lancaster_CI https://t.co/vQQWdrUuqh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03330lhiQjfm"
      },
      "outputs": [],
      "source": [
        "p_tweet = preprocess(tweet)\n",
        "print(p_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Vj6WHkQjfo"
      },
      "source": [
        "For tokenisation, we have a basic custom tokeniser. This is equivalent to the custom tokenisers created last week, but with a pre-compiled regular expression. Alternation is used to separate patterns. Again, you may see fit to edit to suit your needs later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWEO_oHeQjfo"
      },
      "outputs": [],
      "source": [
        "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
        "def custom_tokenise(text):\n",
        "    return tokenise_re.findall(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MxVJKX2Qjfp"
      },
      "source": [
        "Utility methods for displaying/saving tokens list. Can be used for any list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2XiFpKzQjfp"
      },
      "outputs": [],
      "source": [
        "def print_tokens(tokens):\n",
        "    for token in tokens: #iterate tokens and print one per line.\n",
        "        print(token)\n",
        "    print(f\"Total: {len(tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBHI9YwGQjfq"
      },
      "outputs": [],
      "source": [
        "def save_tokens(tokens, outfile):\n",
        "    with open(outfile, 'w', encoding=\"utf-8\") as f:\n",
        "        for token in tokens: #iterate tokens and output to file.\n",
        "            f.write(token + '\\n')\n",
        "        f.write(f\"Total: {len(tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-uTzuPgQjfr"
      },
      "source": [
        "<a name=\"bow\"></a>\n",
        "## Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ch70XqQjfr"
      },
      "source": [
        "Probably the most common NLP feature set, traditionally, is the \"bag of words\". This is a count of each word in the text, disregarding context. Whilst limited, due to the lack of context, a simple bag of words can achieve reasonable results for simple classification tasks, and is often used as a baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGjuOJpMQjfs"
      },
      "source": [
        "First we need to tokenise the text. The tokenisation used will determine what is considered a \"word\", although post processing of the token list could be undertaken, e.g. to filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "joXERV7YQjft"
      },
      "outputs": [],
      "source": [
        "tokens = custom_tokenise(p_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey9ixwQrQjft"
      },
      "outputs": [],
      "source": [
        "print_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWamUj10Qjfu"
      },
      "source": [
        "For simple bag of words, it often makes sense to make the token list all lowercase, so the same word with different casings are merged (e.g. if a word is at the beginning of a sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI4ucB5YQjfv"
      },
      "outputs": [],
      "source": [
        "lower_tokens = [t.lower() for t in tokens] #list comprehension\n",
        "print_tokens(lower_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpKAO6n8Qjfw"
      },
      "source": [
        "Note that Python's `lower()` method is Unicode aware, and will lowercase letters with diacritics and from non-Latin alphabets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5h4q7VeQjfx"
      },
      "outputs": [],
      "source": [
        "\"√Ö√â√é√ë√áŒõ–§\".lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRuqbtHbQjfy"
      },
      "source": [
        "To make a frequency list, we simply place the token list in a [`Counter`](https://docs.python.org/3.7/library/collections.html#counter-objects) object, which extends `dict`, mapping items to frequencies. [NLTK's FreqDist](http://www.nltk.org/_modules/nltk/probability.html#FreqDist), which extends `Counter`, could also be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdj5qOcHQjfz"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "tokens_fql = Counter(lower_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKZqDYcUQjfz"
      },
      "outputs": [],
      "source": [
        "tokens_fql.most_common() #displays frequency list in descending frequency order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpg43RZUQjf0"
      },
      "source": [
        "<a name=\"filtered\"></a>\n",
        "### Filtered list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TGztpmQjf1"
      },
      "source": [
        "At some point we will need to filter the bag of words, e.g. to some top-500 or top-1000 words, as it rarely makes sense to have a feature vector containing all words.\n",
        "\n",
        "The method below uses word frequencies to create a new frequency list containing all in the predefined lists. Including 0s for words not found (dense vector). The vector can be made sparse (remove 0s) with `+counter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZA8UG_sQjf1"
      },
      "outputs": [],
      "source": [
        "def filter_fql(fql, predefined_list):\n",
        "    return Counter({t: fql[t] for t in predefined_list}) #dict comprehension, t: fql[t] is token: freq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UM25jcQjf2"
      },
      "source": [
        "A common feature set (especially for authorship analysis) is function words (aka stop words). Here we use the function word list taken from https://ieeexplore.ieee.org/abstract/document/6234420."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpIja_AjQjf3"
      },
      "outputs": [],
      "source": [
        "def read_list(file):\n",
        "    with open(file) as f:\n",
        "        items = []\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            items.append(line.strip())\n",
        "    return items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuPH-P_tQjf3"
      },
      "outputs": [],
      "source": [
        "fws = read_list(working_folder + \"functionwords.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABJ7zHKGQjf4"
      },
      "outputs": [],
      "source": [
        "fws_fql = filter_fql(tokens_fql, fws)\n",
        "fws_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGjrJeSRQjf5"
      },
      "source": [
        "Remove 0s, and make into sparse vector: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OozYLRcoQjf6"
      },
      "outputs": [],
      "source": [
        "+fws_fql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzRTFxN4Qjf7"
      },
      "source": [
        "Note you need to be careful that the tokenisation matches what is in the function word / stopword list.\n",
        "\n",
        "You could also use NLTK's stopword list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtzG9U7EQjf8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stoplist = stopwords.words('english')\n",
        "print(stoplist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ_WpqjnQjf9"
      },
      "source": [
        "We can remove words from a list (e.g. a stopword list) by iterating the list of words in the frequency list to remove, and 'popping off' ([dict.pop(key,None)](https://docs.python.org/3/library/stdtypes.html?highlight=pop#dict.pop)) each word if present. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBTW-vfrQjf-"
      },
      "outputs": [],
      "source": [
        "def remove_list(fql, to_remove):\n",
        "    filtered = Counter(fql)\n",
        "    for r in to_remove:\n",
        "        filtered.pop(r,None)        \n",
        "    return filtered\n",
        "\n",
        "filtered = remove_list(tokens_fql, stoplist)\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTMxCWUVQjgA"
      },
      "source": [
        "<a name=\"wordn\"></a>\n",
        "### Word n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Ky88EeQjgB"
      },
      "source": [
        "To get some context for words, we can use sequences of words instead of single words, these are known as word n-grams. bigrams (2-grams) and trigrams (3-grams) are popular. One issue with word n-grams is their sparsity. It's a good idea to reduce the size of the vocabulary as much as possible, e.g. digits and dates could be mapped to single tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOodIkNMQjgC"
      },
      "source": [
        "Whether a token appears at the start or end of the text (or could be sentence) can be useful, so we can introduce buffer markers at the start and end to indicate this.\n",
        "\n",
        "Note also that n-grams should be created with a sliding window over the text, i.e. the first word bigram is the first and second word, the second bigram is the second and third word.\n",
        "\n",
        "The method below is a generic method for turning a list of tokens into an n-gram list, adding the buffer characters either side, and moving a sliding window of size n across the text and providing a list of n-grams. Check your understanding of how this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTBskh9_QjgC"
      },
      "outputs": [],
      "source": [
        "def ngrams(tokens, n, sep = \"_\", buffer=\"^\"):\n",
        "    buffered = [buffer] * (n-1) + tokens + [buffer] * (n-1) #add buffer either side to denote start and end\n",
        "    return [sep.join(buffered[i:i+n]) for i in range(len(buffered)-n+1)] #list comprehension creating merged string of n chars, with a window of n through string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjhDBCncQjgD"
      },
      "outputs": [],
      "source": [
        "word_bigrams = ngrams(lower_tokens,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nccl3qA7QjgD"
      },
      "outputs": [],
      "source": [
        "word_bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLxb6sFjQjgE"
      },
      "outputs": [],
      "source": [
        "word_bigrams_fql = Counter(word_bigrams)\n",
        "word_bigrams_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9H1mmEnQjgF"
      },
      "source": [
        "**Quick task:** Produce a frequency list of word trigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG_50c28QjgG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P0yaevAQjgG"
      },
      "source": [
        "<a name=\"chars\"></a>\n",
        "## Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw44aB_jQjgG"
      },
      "source": [
        "Just looking at characters as features is a simple (yet often powerful) way of processing text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5WFWqUeQjgH"
      },
      "outputs": [],
      "source": [
        "print(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0eK7nHaQjgI"
      },
      "outputs": [],
      "source": [
        "print(p_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGmSUjNvQjgJ"
      },
      "source": [
        "We probably don't want the artificial hashtag, mention, and url markers, we could keep these as is, replace with single chars, or just remove them. Below we just remove them. We often have different pre-processing for different features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9YM-S9BQjgJ"
      },
      "outputs": [],
      "source": [
        "def preprocess_remove(text):\n",
        "    r_text = hashtag_re.sub(\"\",text)\n",
        "    r_text = mention_re.sub(\"\",r_text)\n",
        "    r_text = url_re.sub(\"\",r_text)\n",
        "    r_text = ftfy.fix_text(r_text)\n",
        "    return r_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBJBQGU9QjgJ"
      },
      "outputs": [],
      "source": [
        "r_tweet = preprocess_remove(tweet)\n",
        "print(r_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UUid-gQjgK"
      },
      "source": [
        "Note, extra spaces are included now, how could you preprocess the text further to reduce multiple spaces to a single space?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jH3EdukQjgK"
      },
      "source": [
        "In Python a string is just a sequence (list) of characters, so we can just iterate through the characters as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y4cYa31QjgK"
      },
      "outputs": [],
      "source": [
        "for char in r_tweet:\n",
        "    print(char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkg4KyRYQjgL"
      },
      "source": [
        "We can make this count the frequency of each character easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrH2j0g-QjgL"
      },
      "outputs": [],
      "source": [
        "char_fql = Counter(r_tweet)\n",
        "char_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXSfAeLXQjgM"
      },
      "source": [
        "This appears to work well, **but this should be used with caution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uju_Kx_QjgM"
      },
      "outputs": [],
      "source": [
        "test = \"Remember the spicy jalapen\\u0303o\"\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7afuIsJQjgM"
      },
      "outputs": [],
      "source": [
        "for i, char in enumerate(test):\n",
        "    print(i,char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0TLeC8EQjgO"
      },
      "source": [
        "Notice the  ÃÉ separated from the n because it is a separate codepoint (combining). It is placed over the space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsIMBihGQjgP"
      },
      "source": [
        "This looks even worse if we view the characters as a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l8cHphiQjgP"
      },
      "outputs": [],
      "source": [
        "print(list(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JIA_HXQjgP"
      },
      "source": [
        "See the  ÃÉ over the single quote mark. Nasty! ü§¢\n",
        "\n",
        "The combining codepoint combines with whatever the character before is, and in this case it's displayed as the quote mark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t6a6VFkQjgQ"
      },
      "source": [
        "We need to be careful how we define \"character\". In Python a 'character' is a single Unicode codepoint. When in reality, we should be looking for \"graphemes\", i.e. displayed single characters (which may be a cluster of codepoints): https://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvY87AzHQjgQ"
      },
      "source": [
        "As you saw last week, we can use regular expressions to find these graphemes, but Python's default regular expression library (re), whilst being Unicode aware, does not deal with Unicode particularly well. The [regex library](https://pypi.org/project/regex/) has better support, providing the use of unicode categories: https://www.regular-expressions.info/unicode.html, including `\\X` to match single graphemes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2vXYhkQjgQ"
      },
      "outputs": [],
      "source": [
        "import regex\n",
        "char_regex = regex.compile(r'\\X')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L6DGfZNQjgQ"
      },
      "outputs": [],
      "source": [
        "chars = char_regex.findall(test)\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODXrJOj6QjgS"
      },
      "source": [
        "This nicely separates √± as single \"character\". We can put this into a frequency list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6HKWWJiQjgS"
      },
      "outputs": [],
      "source": [
        "char_fql = Counter(chars)\n",
        "char_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M3ha4BTQjgT"
      },
      "source": [
        "Even more \"fun\" can be had with emojis, which can contain numerous codepoints, particularly joined with zero-width-joiners: https://unicode.org/emoji/charts/emoji-zwj-sequences.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwApTvn3QjgT"
      },
      "outputs": [],
      "source": [
        "emoji_test = \"This is one emoji: \\U0001F468\\u200D\\U0001F469\\u200D\\U0001F467\\u200D\\U0001F466\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oINHxHYoQjgT"
      },
      "outputs": [],
      "source": [
        "print(emoji_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywBBXcfcQjgU"
      },
      "outputs": [],
      "source": [
        "test_matches = char_regex.findall(emoji_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SWX-1wuQjgU"
      },
      "outputs": [],
      "source": [
        "for match in test_matches:\n",
        "    print(match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nmWvWCuQjgV"
      },
      "source": [
        "Another library, [grapheme](https://pypi.org/project/grapheme/), also provides funtionality to deal with these graphemes like characters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grapheme"
      ],
      "metadata": {
        "id": "O7TrnmVcVSxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-d5NMOeQjgV"
      },
      "outputs": [],
      "source": [
        "import grapheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voQifQxZQjgV"
      },
      "outputs": [],
      "source": [
        "graphemes = list(grapheme.graphemes(emoji_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45h0eDnFQjgV"
      },
      "outputs": [],
      "source": [
        "for g in graphemes:\n",
        "    print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt68XKekQjgW"
      },
      "outputs": [],
      "source": [
        "char_fql = Counter(graphemes)\n",
        "char_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4QX3phBQjgW"
      },
      "source": [
        "Note, when composite graphemes are printed in a list/tuple, they're expanded for some reason (if you know why, please tell me!). As can be seen, this is just a display issue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQuhifOqQjgX"
      },
      "outputs": [],
      "source": [
        "for char in char_fql.most_common():\n",
        "    print(\"{}\\t{}\".format(char[0], char[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5oC4qkPQjgX"
      },
      "source": [
        "<a name=\"charn\"></a>\n",
        "### Character n-grams\n",
        "\n",
        "We can also look at sequences of characters, though be aware that these will overlap with words and other features (double counting).\n",
        "\n",
        "You have everything you need to do this (remember the n-grams function is generic).\n",
        "\n",
        "**Task:** Produce character trigrams for the tweet. You don't need a separator for chars, so the first trigram should be '^^T'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj8mflZTQjgY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyl_F-9tQjgY"
      },
      "source": [
        "<a name=\"ann\"></a>\n",
        "## Annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlInHCKIQjgY"
      },
      "source": [
        "As discussed in the lecture, various levels of annotation are available to add on top of the tokens. These are extra levels of information that can be used as features for various NLP tasks. Lemmatisation is one option available and straightforward to [implement with nltk](http://www.nltk.org/book/ch03.html#lemmatization).\n",
        "\n",
        "Part-of-speech (POS) tags are probably the most used form of annotation, certainly for classification tasks. NLTK provides a POS tagger using the standard [Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Tokenised text can be POS tagged easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA6wakviQjgZ"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('maxent_treebank_pos_tagger')\n",
        "nltk.download('averaged_perceptron_tagger') # check how uses penn and look at alternatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwoDUammQjgZ"
      },
      "outputs": [],
      "source": [
        "pos_tagged = nltk.pos_tag(tokens)\n",
        "pos_tagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovNLg0YYQjga"
      },
      "source": [
        "What do you think of the accuracy of the POS tags on this small sample? You can see a description of each POS tag with the below. Note, we should POS tag the tokens without making them lowercase first, as POS taggers will use capital letters, e.g. for proper nouns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJzXlH5PQjga"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub7wtSejQjgb"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"tagsets\")\n",
        "nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnPQmVngQjgb"
      },
      "source": [
        "To create a POS frequency list is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ2aBRI2Qjgb"
      },
      "outputs": [],
      "source": [
        "pos = [tag[1] for tag in pos_tagged]\n",
        "pos_fql = Counter(pos)\n",
        "pos_fql.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB8CZkYlQjgc"
      },
      "source": [
        "**Task:** Try to make improvements to the POS tagging by changing the preprocessing and tokenisation. As a minimum, try using NLTK's default tokeniser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2fYs6w_Qjge"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azgFfC7WQjge"
      },
      "source": [
        "**Advanced task:** \n",
        "\n",
        "Developing a POS tagger that is capable of dealing well with the intricacies of user generated content (e.g. Twitter) text is difficult, although there have been attempts, e.g. http://www.cs.cmu.edu/~ark/TweetNLP/. One option is to post-process the POS tagged text to fix the main issues.\n",
        "\n",
        "Define a function that takes the POS tagged text and post-processes the output to add new tags for mentions, hashtags, urls, emojis, and anything else you can see to fix with simple rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yh0Ze6hQjgf"
      },
      "source": [
        "<a name=\"other\"></a>\n",
        "## Other features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUqhzGQNQjgf"
      },
      "source": [
        "Many other features can be calculated over the text, token stream, or other feature frequency lists. Some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtDLfU8yQjgf"
      },
      "outputs": [],
      "source": [
        "length_chars = len(tweet) #length of text in chars\n",
        "length_tokens = len(tokens) #length of text in tokens\n",
        "print(length_chars)\n",
        "print(length_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomMrtFbQjgf"
      },
      "source": [
        "Average word length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Wx-Shc5HQjgg"
      },
      "outputs": [],
      "source": [
        "avg_word_length = sum([len(tok) for tok in tokens])/length_tokens #make a list of lengths per token, sum and divide by number of tokens\n",
        "print(avg_word_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE1mLJDqQjgh"
      },
      "source": [
        "Various vocabulary measures are available that represent how varied and large the vocabulary is of the text.\n",
        "\n",
        "We need to know the number of **word types** present, this is the number of words, counting multiple instances (tokens) of the same word once. This is simply the size of the frequency list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfU-p463Qjgh"
      },
      "outputs": [],
      "source": [
        "length_types = len(tokens_fql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ASzHG9Qjgi"
      },
      "source": [
        "Type Token Ratio (TTR) is a popular vocabulary measure, simply dividing the number of types by the number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnXVc6X7Qjgi"
      },
      "outputs": [],
      "source": [
        "ttr = length_types / length_tokens #type token ratio (ttr)\n",
        "print(ttr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmBEQGxHQjgk"
      },
      "source": [
        "TTR is not comparable over texts of very different lengths, instead use something like Moving-Average Type-Token Ratio (MATTR): https://doi.org/10.1080/09296171003643098\n",
        "\n",
        "**Advanced task**: Reading the above linked paper, implement MATTR."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lqg_vrJZS5nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfxAjorpQjgk"
      },
      "source": [
        "Other vocabulary measures look at the number of hapaxes (words types which only appear once), below a simple hapax ratio is calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX0rNaq_Qjgk"
      },
      "outputs": [],
      "source": [
        "hapaxes = list(tokens_fql.values()).count(1) #convert frequencies to list and count 1s.\n",
        "hapax_ratio = hapaxes / length_types\n",
        "print(hapax_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAnls1lpQjgm"
      },
      "source": [
        "There are many other features that could be implemented. Readability metrics could be calculated, most of which require a count of syllables. Counting syllables is actually [quite an involved task](https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word/4103234), especially for user generated content, and multi-lingual data. [Big Phoney](https://github.com/repp/big-phoney) is one option that seems promising (based on some limited testing). An **Advanced Task** would be to implement one or more the readability measures (e.g. [*Flesch reading ease*](https://en.wikipedia.org/wiki/Flesch‚ÄìKincaid_readability_tests))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6GqZUebQjgn"
      },
      "source": [
        "Counting and splitting text into sentences is also needed for some features. This is quite simple to do with NLTK, as below. Though be aware, like other segmentation tasks, doing this accurately with user generated content is not straight-forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVRUK34qQjgn"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(p_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fliq7dC-Qjgo"
      },
      "source": [
        "<a name=\"docs\"></a>\n",
        "## Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezWnIwxvQjgo"
      },
      "source": [
        "So far we have been utilising a single line of text (Tweet) to demonstrate feature extraction. However, we will often be dealing with larger texts consisting of lines of texts (e.g. paragraphs or sets of Tweets), we can call these documents. We normally do not want sequence features (e.g. n-grams) to go across line boundaries within a document. Hence we process and extract features per line of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hjxapslQjgp"
      },
      "source": [
        "To make things a little easier, we create a `Document` class which holds the features of a document (and any metadata provided). Features are calculated with the `extract_featues` function, which takes in a list (iterable) of texts (which could be lines in a text, or individual tweets from a user). Currently, just tokens are counted (i.e. Bag of Words), and a single method to demonstrate how to return Document level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvIhVAOKQjgp"
      },
      "outputs": [],
      "source": [
        "class Document:\n",
        "    def __init__(self, meta={}):\n",
        "        self.meta = meta\n",
        "        self.tokens_fql = Counter() #empty counter, ready to be added to with Counter.update.\n",
        "        \n",
        "    def extract_features(self, texts): #document should be iterable text lines, e.g. read in from file.\n",
        "        for text in texts:\n",
        "            p_text = preprocess(text)\n",
        "            tokens = custom_tokenise(p_text)\n",
        "            lower_tokens = [t.lower() for t in tokens]\n",
        "            self.tokens_fql.update(lower_tokens) #updating Counter counts items in list, adding to existing Counter items.\n",
        "            \n",
        "    def get_ttr(self): #type token ratio\n",
        "        length_types = len(self.tokens_fql)\n",
        "        length_tokens = sum(self.tokens_fql.values())\n",
        "        return length_types / length_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cmici50Qjgp"
      },
      "source": [
        "To utilise this, we simply create a Document, and add text to it. An example using the existing tweet we've been using is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvHYvObbQjgq"
      },
      "outputs": [],
      "source": [
        "tweet_doc = Document()\n",
        "tweet_doc.extract_features([tweet])\n",
        "print(tweet_doc.tokens_fql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgqY3lt3Qjgq"
      },
      "source": [
        "<a name=\"mps\"></a>\n",
        "## MPs Dataset\n",
        "In order to play with features, a collection of Tweets from MP accounts is provided in the `mps` folder. These are plain text files for each user, split into Labour and Conservative. These Tweets were collected a while back, so the list of MPs (some are no longer MPs, or have left Labour or Conservatives) and Tweets is not current. You could use what you've learnt from week 14 (data collection) to gather a list of MPs from https://www.politics-social.com/list/name, and download their latest tweets. More MP data also here: https://www.theyworkforyou.com/mps/.\n",
        "\n",
        "The corpus can be read into Documents as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G5buvTmQjgq"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, splitext, split\n",
        "\n",
        "\n",
        "def import_party_folder(party):\n",
        "    folder = working_folder + \"mps/\" + party\n",
        "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
        "    for tf in textfiles:\n",
        "        username = splitext(split(tf)[1])[0] #extract just username from filename.\n",
        "        print(\"Processing \" + username)\n",
        "        doc = Document({'username': username, 'party': party}) #include metadata\n",
        "        with open(tf) as f:\n",
        "            tweets = f.readlines()\n",
        "        doc.extract_features(tweets)\n",
        "        yield doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6uQcHcoQjgr"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus.extend(import_party_folder(\"labour\"))\n",
        "corpus.extend(import_party_folder(\"conservative\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoPiMGhJQjgr"
      },
      "source": [
        "We now have a **corpus** of MPs on Twitter we can use for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA5jGYNkQjgr"
      },
      "outputs": [],
      "source": [
        "for doc in corpus:\n",
        "    print(doc.meta['username'], doc.meta['party'], sum(doc.tokens_fql.values()),sep=\", \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuuUILtpQjgr"
      },
      "source": [
        "<a name=\"corpus\"></a>\n",
        "## Corpus analysis\n",
        "We can compare corpora or sub-corpora to start to gain insights into language differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTmlcX3Qjgs"
      },
      "source": [
        "Our frequency lists (FQLs) are stored as [`Counters`](https://docs.python.org/3.7/library/collections.html#counter-objects), which can be merged easily by just adding them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJznpEXCQjgs"
      },
      "outputs": [],
      "source": [
        "def merge_fqls(fqls):\n",
        "    merged = Counter()\n",
        "    for fql in fqls:\n",
        "        merged += fql\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKpbDjqgQjgt"
      },
      "source": [
        "Create a sub-corpus, one for Conservative MPs, another for Labour MPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9e4fCQDQjgt"
      },
      "outputs": [],
      "source": [
        "con_fql = merge_fqls([doc.tokens_fql for doc in corpus if doc.meta['party']==\"conservative\"])\n",
        "lab_fql = merge_fqls([doc.tokens_fql for doc in corpus if doc.meta['party']==\"labour\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWwhTwTJQjgt"
      },
      "outputs": [],
      "source": [
        "con_size = sum(con_fql.values())\n",
        "lab_size = sum(lab_fql.values())\n",
        "print(con_size,lab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ygTApRQjgu"
      },
      "source": [
        "We can start analysing the most frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J9CPLjtQjgu"
      },
      "outputs": [],
      "source": [
        "print(lab_fql.most_common(20))\n",
        "print(con_fql.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wks7U0ISQjgu"
      },
      "source": [
        "And even create a basic [word cloud](https://github.com/amueller/word_cloud)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BK_HO7bQjgu"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_wordcloud(words):\n",
        "    wordcloud = WordCloud().generate_from_frequencies(words)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYPYNpB9Qjgv"
      },
      "outputs": [],
      "source": [
        "create_wordcloud(con_fql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp1-63kxQjgv"
      },
      "outputs": [],
      "source": [
        "create_wordcloud(lab_fql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd-qyu3VQjgv"
      },
      "source": [
        "Common words dominate. How could we remove these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aZdnMJAQjgw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrL4dDiQjgw"
      },
      "source": [
        "To normalise the frequencies, we can simply divide by the number of tokens, to gain relative frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ3P_JIjQjgw"
      },
      "outputs": [],
      "source": [
        "def relative_freqs(fql):\n",
        "    size = sum(fql.values())\n",
        "    return {term: fql[term]/size for term in fql}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv-lXmJfQjgw"
      },
      "outputs": [],
      "source": [
        "con_rel = relative_freqs(con_fql)\n",
        "lab_rel = relative_freqs(lab_fql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Ep4deuQjgw"
      },
      "source": [
        "To do a \"Key words\" comparison between the sub-corpora, we can utilise [*Log Ratio*](http://cass.lancs.ac.uk/log-ratio-an-informal-introduction/), which is the binary log of the relative risk (ratio between relative frequencies). Other significance tests and effect size measures can be used: http://ucrel.lancs.ac.uk/llwizard.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPKqjtkEQjgx"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "\n",
        "#Calculates log ratio for terms in corpus1, compared to corpus2.\n",
        "#we pass the corpus sizes for ease.\n",
        "#If the term is not present in corpus2, we make the frequency 0.5.\n",
        "def log_ratio(corpus1, corpus1_size, corpus2, corpus2_size, min_freq1=0, min_freq2=0):\n",
        "    return {term: log((corpus1[term]/corpus1_size)/((corpus2[term] if corpus2[term] else 0.5)/corpus2_size),2) for term in corpus1 if corpus1[term] >= min_freq1 and corpus2[term] >= min_freq2}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above method is a dict comprehension one-liner, which may be difficult to interpret. The below method does exactly the same as the above, but is split over multiple lines to ease readability and understanding. "
      ],
      "metadata": {
        "id": "P8su00StPzuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_5LKzSSQjgx"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "\n",
        "#Calculates log ratio for terms in corpus1, compared to corpus2.\n",
        "#we pass the corpus sizes for ease.\n",
        "#If the term is not present in corpus2, we make the frequency 0.5.\n",
        "def log_ratio(corpus1, corpus1_size, corpus2, corpus2_size, min_freq1=0, min_freq2=0):\n",
        "    lrs = dict()\n",
        "    for term in corpus1:\n",
        "      if corpus1[term] >= min_freq1 and corpus2[term] >= min_freq2:\n",
        "        rel_freq1 = corpus1[term]/corpus1_size\n",
        "        if corpus2[term]:\n",
        "          freq2 = corpus2[term]\n",
        "        else:\n",
        "          freq2 = 0.5\n",
        "        rel_freq2 = freq2/corpus2_size\n",
        "        lr = log(rel_freq1/rel_freq2, 2)\n",
        "        lrs[term] = lr\n",
        "\n",
        "    return lrs\n",
        "        \n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtVRBJguQjgx"
      },
      "source": [
        "Calculate the terms from Conservative MPs with the biggest log ratio compared to terms from Labour MPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uW43P3XQjgx"
      },
      "outputs": [],
      "source": [
        "con_lr = log_ratio(con_fql, con_size, lab_fql, lab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmW_Mm3pQjgy"
      },
      "source": [
        "We can sort our list of terms by this log ratio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B426QT6jQjgy"
      },
      "outputs": [],
      "source": [
        "sorted_terms = sorted(con_lr.items(), key=lambda x: x[1], reverse=True)\n",
        "print(sorted_terms[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_Fs5iDQjg0"
      },
      "source": [
        "and create a word cloud using the log ratios, instead of frequencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp0MsaGUQjg0"
      },
      "outputs": [],
      "source": [
        "create_wordcloud(con_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz51gm-bQjg0"
      },
      "source": [
        "and the other way round:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hh3CkrOQjg1"
      },
      "outputs": [],
      "source": [
        "lab_lr = log_ratio(lab_fql, lab_size, con_fql, con_size)\n",
        "sorted_terms = sorted(lab_lr.items(), key=lambda x: x[1], reverse=True)\n",
        "print(sorted_terms[:20])\n",
        "create_wordcloud(lab_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6njOZsjfQjg1"
      },
      "source": [
        "Some interesting terms appear, but with a small number of authors, some terms will be prominent from one MP, boosting the frequency in the sub-corpus. You can set a minimum frequency for a term to appear in each corpus. The below sets a minimum frequency of 5 in each corpus, which will rule out words that only appear in one sub corpus. You can change these minimum frequencies (e.g. 1,1). Notice you will get quite different words highlighted in the wordcloud."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab_lr = log_ratio(lab_fql, lab_size, con_fql, con_size,5,5)\n",
        "sorted_terms = sorted(lab_lr.items(), key=lambda x: x[1], reverse=True)\n",
        "print(sorted_terms[:20])\n",
        "create_wordcloud(lab_lr)"
      ],
      "metadata": {
        "id": "gru_g8uMXL9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-ePPuKEQjg2"
      },
      "source": [
        "<a name=\"tfidf\"></a>\n",
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epGHmuUZQjg2"
      },
      "source": [
        "As discussed in the lecture, TF-IDF is a commonly used normalisation method which considers the term frequency along with how many documents in the corpus the term appears in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qJlD-bXQjg2"
      },
      "outputs": [],
      "source": [
        "#doc is a Counter representing an fql from a document.\n",
        "def tf(term, doc):\n",
        "    return doc[term] / sum(doc.values()) #term freq / total terms (relative term freq)\n",
        "\n",
        "def num_containing(term, corpus):\n",
        "    return sum(1 for doc in corpus if term in doc) #counts docs in corpus containing term.\n",
        "\n",
        "#1 added to numerator and denominator is for preventing division by zero. Equivalent of an extra document containing all terms once.\n",
        "def idf(term, corpus):\n",
        "    n_t = num_containing(term,corpus)\n",
        "    return log((len(corpus)+1) / ((n_t) + 1))\n",
        "    \n",
        "def tfidf(term, doc, corpus):\n",
        "    return tf(term, doc) * idf(term, corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lovkoWdQjg3"
      },
      "source": [
        "We can calculate the TF-IDF for every term for every MP in the corpus. By listing the terms with the highest TF-IDF, we can look at terms that are used by that MP frequently, but only used by that MP alone, or a small number of MPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1kM5MIQjg3"
      },
      "outputs": [],
      "source": [
        "corpus_fqls = [doc.tokens_fql for doc in corpus]\n",
        "for doc in corpus:\n",
        "    print(doc.meta['username'], doc.meta['party'])\n",
        "    scores = {term: tfidf(term,doc.tokens_fql,corpus_fqls) for term in doc.tokens_fql}\n",
        "    sorted_terms = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for term, score in sorted_terms[:5]:\n",
        "        print(\"\\tToken: {}, TF-IDF: {}\".format(term, round(score, 5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5DyYEw5Qjg3"
      },
      "source": [
        "<a name=\"ex\"></a>\n",
        "## Exercise\n",
        "Use the MPs data provided to conduct some further feature extraction and analysis. Write code to answer the following questions:\n",
        "\n",
        "1. Which MP in the dataset has the highest average token length?\n",
        "2. What are the 5 **key** part-of-speech tags overused by Labour MPs compared to Conservative MPs?\n",
        "3. What hashtags does Jeremy Corbyn use frequently, which aren't used widely by the rest of the Labour party MPs in the dataset (TF-IDF)?\n",
        "4. **Advanced:** What are the **key** adjectives overused by Boris Johnson compared to other MPs?\n",
        "4. **Advanced:** If you want to go further, devise your own research question, either using the MP data provided, collecting a new MP dataset, or on different data.\n",
        "\n",
        "You may need to pre-process and tokenise the text differently. Re-use the code above, including adapting the Document class, adding/editing preprocessing, tokenisation, and feature extraction.\n",
        "\n",
        "If you prefer, you can create a new notebook for the exercise work. The methods and imports above are provided in a Python file too: `features.py`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yhor2_ZvTioV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "413-wk18-features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}