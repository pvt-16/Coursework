{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIpcfPwJXgLA"
      },
      "source": [
        "# SCC.413 Applied Data Mining\n",
        "# Week 18\n",
        "# Classification 2 (using custom Document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLmSoVHIXgLG"
      },
      "source": [
        "## Contents\n",
        "* [Introduction](#intro)\n",
        "* [Preamble](#preamble)\n",
        "* [Document object](#doc)\n",
        "    - [Dataset](#data)\n",
        "    - [Document processor](#processor)\n",
        "* [Feature Union](#union)\n",
        "* [Classifying other labels](#other)\n",
        "* [Exercise](#ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MBtqfQxXgLH"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "## Introduction\n",
        "\n",
        "In the the first classification notebook scikit-learn was used to classify Twitter users as male or female. Vectorization was done via sklearn's CountVectorizer. At the end of the lab exercise, the preprocessing and tokenisation used by CountVectorizer were customised to give greater control of how the text was processed and features extracted. This actually allows for quite a number of features to be implemented, e.g.:\n",
        "\n",
        "- POS tags, by pos-tagging during tokenisation, and returning the pos-tags instead of words.\n",
        "- Function words, by setting the vocabulary of CountVectorizer to a function word list.\n",
        "- Hashtags, mentions and/or emojis, by only returning these in the token list.\n",
        "- Characters/graphemes, rather than codepoints, by \"tokenising\" with \"\\X\".\n",
        "\n",
        "But we are limited to counting things. Another issue is that the processing will be done multiple times each time the pipeline is ran (e.g. for gridsearch).\n",
        "\n",
        "In this lab you will learn how to process texts in advance, saving the token lists and frequency lists produced, and allowing for other features to be extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opy_3ksmXgLI"
      },
      "source": [
        "<a name=\"preamble\"></a>\n",
        "## Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sMeR-FQXgLJ"
      },
      "source": [
        "First, we do all the imports needed in one go. You may import other packages as needed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "id": "C6oy4d1KYKD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCT7CiO9XgLK"
      },
      "outputs": [],
      "source": [
        "import ftfy\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import Binarizer, StandardScaler\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from os import listdir, makedirs\n",
        "from os.path import isfile, join, splitext, split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in previous notebooks, you should upload all of the provided files to a Google Drive folder, you can then access these files from your Python code. See also the files tab."
      ],
      "metadata": {
        "id": "tS7ckSyCBCoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "ydCluoej2UY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the folder we are working from as a variable for easy access. You may need to edit the path to match your own."
      ],
      "metadata": {
        "id": "trooi5ZMzSM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_folder = '/content/gdrive/MyDrive/413/wk19/'"
      ],
      "metadata": {
        "id": "c1YsiHLvzKeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code adds the working folder to the system path, so you can import Python files from this folder."
      ],
      "metadata": {
        "id": "ktoDdLMkBVMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(working_folder)"
      ],
      "metadata": {
        "id": "Cq_7TvOW34v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFA0TSwaXgLN"
      },
      "source": [
        "A couple of functions for showing classifier results (from first classification lab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqTX--rXgLO"
      },
      "outputs": [],
      "source": [
        "def print_cv_scores_summary(name, scores):\n",
        "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))\n",
        "    \n",
        "def confusion_matrix_heatmap(cm, index):\n",
        "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
        "    dims = (5, 5)\n",
        "    fig, ax = plt.subplots(figsize=dims)\n",
        "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
        "    ax.set_ylabel('Actual')    \n",
        "    ax.set_xlabel('Predicted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dJpsXgZXgLP"
      },
      "source": [
        "<a name=\"doc\"></a>\n",
        "## Document object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4NTN2xXgLP"
      },
      "source": [
        "Next we define our document object, along with preprocessing tokenisation methods. These are taken (and slightly extended) from the Feature Extraction lab. The document will represent an instance in our classifier, e.g. it could be a collection of a user's Tweets, a single tweet, a longer article, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9be1SyMXgLQ"
      },
      "outputs": [],
      "source": [
        "hashtag_re = re.compile(r\"#\\w+\")\n",
        "mention_re = re.compile(r\"@\\w+\")\n",
        "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
        "\n",
        "def preprocess(text):\n",
        "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
        "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
        "    p_text = url_re.sub(\"[url]\",p_text)\n",
        "    p_text = ftfy.fix_text(p_text)\n",
        "    return p_text\n",
        "\n",
        "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
        "def tokenise(text):\n",
        "    return tokenise_re.findall(text)\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, meta={}):\n",
        "        self.meta = meta\n",
        "        self.tokens_fql = Counter() #empty Counter, ready to be added to with Counter.update.\n",
        "        self.ht_fql = Counter()\n",
        "        self.num_tokens = 0\n",
        "        \n",
        "    def extract_features_from_text(self, text):\n",
        "        hts = hashtag_re.findall(text)\n",
        "        self.ht_fql.update([ht.lower() for ht in hts])\n",
        "        p_text = preprocess(text)\n",
        "        tokens = tokenise(p_text)\n",
        "        lower_tokens = [t.lower() for t in tokens]\n",
        "        self.num_tokens += len(lower_tokens)\n",
        "        self.tokens_fql.update(lower_tokens) #updating Counter counts items in list, adding to existing Counter items.\n",
        "        \n",
        "    def extract_features_from_texts(self, texts): #texts should be iterable text lines, e.g. read in from file.\n",
        "        for text in texts:\n",
        "            extract_features_from_text(text)\n",
        "            \n",
        "    def average_token_length(self):\n",
        "        sum_lengths = 0\n",
        "        for key, value in self.tokens_fql.items():\n",
        "            sum_lengths += len(key) * value\n",
        "        return sum_lengths / self.num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i3xoijMXgLR"
      },
      "source": [
        "<a name=\"data\"></a>\n",
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFxI3l0fXgLR"
      },
      "source": [
        "We use the Twitter celebs again (this time with extra users from around the world), but from JSON files with further metadata available. The method below reads in the json file, extracts the metadata for the user, creates a new Document representing the user, and adds all of the Tweets (extracting features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjD_ikdXXgLS"
      },
      "outputs": [],
      "source": [
        "def import_celebs_json(folder):\n",
        "    jsonfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".json\")]\n",
        "    for jf in jsonfiles:\n",
        "        with open(jf) as f:\n",
        "            data = json.load(f)\n",
        "            handle = data['handle']\n",
        "            gender = data['gender']\n",
        "            age_range = data['age_range']\n",
        "            english = data['english']\n",
        "        print(\"Processing \" + handle)\n",
        "        doc = Document(meta={'handle': handle, 'gender': gender, 'age_range': age_range, 'english': english}) #include metadata\n",
        "        for tweet in data['tweets']:\n",
        "            doc.extract_features_from_text(tweet['full_text'])\n",
        "        yield doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MnCWf-zXgLT"
      },
      "source": [
        "Build the corpus by processing the folder of celebs. This will take a little while, but it should only need doing once (unless you change the Document class). We are using just the gb users here, but you can use one of the bigger datasets if you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm-McaU2XgLT"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "corpus.extend(import_celebs_json(working_folder + \"celebs-gb-json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9is6xHEjXgLW"
      },
      "source": [
        "We use the gender metadata from the Documents as our label/class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsMAAYU7XgLX"
      },
      "outputs": [],
      "source": [
        "y = [d.meta['gender'] for d in corpus]\n",
        "X = corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwfuxanOXgLZ"
      },
      "source": [
        "We can also perform the usual train/test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2ML3FJdXgLa"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
        "print(len(X_train), len(X_test))\n",
        "print(len(y_train), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlApRNl0XgLb"
      },
      "source": [
        "<a name=\"processor\"></a>\n",
        "### Document processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwD92mbcXgLc"
      },
      "source": [
        "Now we have our train/test instances in our custom class to hold the features, we need to build a custom `Transformer` which takes in one dataset and returns a new dataset. Here we need to take in a list of `Document` objects and transform it into a set of features. We build a simple class for this, which overrides the transform method. The intention is for a list of `Document` objects to be passed into the transformer, and a parameter-defined (callable) method is used to extract the featuress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKrr2L4CXgLd"
      },
      "outputs": [],
      "source": [
        "class DocumentProcessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, process_method):\n",
        "        self.process_method = process_method\n",
        "    \n",
        "    def fit(self, X, y=None): #no fitting necessary, although could use this to build a vocabulary for all documents, and then limit to set (e.g. top 1000).\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        for document in documents:\n",
        "            yield self.process_method(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLOr0Ad_XgLe"
      },
      "source": [
        "Below are some example methods for returning the token frequency list, hashtag frequency list, or some text statistics from the `Document`. These can be edited and added to as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9gAHTnbXgLf"
      },
      "outputs": [],
      "source": [
        "def get_tokens_fql(document):\n",
        "    return document.tokens_fql\n",
        "\n",
        "def get_ht_fql(document):\n",
        "    return document.ht_fql\n",
        "\n",
        "def get_text_stats(document):\n",
        "    ttr = len(document.tokens_fql) / document.num_tokens\n",
        "    return {'avg_token_length': document.average_token_length(), 'ttr': ttr }\n",
        "\n",
        "def read_list(file):\n",
        "    with open(file) as f:\n",
        "        items = []\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            items.append(line.strip())\n",
        "    return items\n",
        "\n",
        "fws = read_list(working_folder + \"functionwords.txt\")\n",
        "\n",
        "def get_fws_fql(document):\n",
        "    fws_fql = Counter({t: document.tokens_fql[t] for t in fws}) #dict comprehension, t: fql[t] is token: freq.\n",
        "    return +fws_fql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSsS71vFXgLg"
      },
      "source": [
        "We build a pipeline with the `DocumentProcessor` as the first step, extract hashtag frequencies as features. The output from the new `DocumentProcessor` is then passed to a [`DictVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which transforms the features into a vector (scipy.sparse matrix), which can be used with other sklearn steps. To demontrate, we pass to a logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPlg69EqXgLg"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    ('processor', DocumentProcessor(process_method = get_ht_fql)),\n",
        "    ('vectorizer', DictVectorizer()),\n",
        "    ('standardize', StandardScaler(with_mean=False)),\n",
        "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNs6UZlgXgLh"
      },
      "outputs": [],
      "source": [
        "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
        "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
        "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
        "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
        "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15sFGEmzXgLh"
      },
      "source": [
        "<a name=\"union\"></a>\n",
        "## Feature Union\n",
        "\n",
        "All the pipelines we have used so far have been simple linear sequences of steps. With [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) we can have complex branched sequences, where different features, or different parts of the text are processed separately. The results are then concatenated together into a composite vector. We could, for example, read in a user's bio separately from the rest of their text, we could even utilise metadata (such as usernames to predict gender), or other data such as profile images.\n",
        "\n",
        "Below we utilise different featuresets available from the Document instances and concatenate them together simply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF5XgrzgXgLi"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    ('text_union', FeatureUnion(\n",
        "        transformer_list = [\n",
        "            ('ht_features', Pipeline([\n",
        "                ('ht_processor', DocumentProcessor(process_method = get_ht_fql)),\n",
        "                ('ht_vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "            ('fws_features', Pipeline([\n",
        "                ('fws_processor', DocumentProcessor(process_method = get_fws_fql)),\n",
        "                ('fws_vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "            ('stats_features', Pipeline([\n",
        "                ('stats_processor', DocumentProcessor(process_method = get_text_stats)),\n",
        "                ('stats_vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "        ],\n",
        "    )),\n",
        "    ('standardize', StandardScaler(with_mean=False)),\n",
        "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPmvEQ3bXgLi"
      },
      "outputs": [],
      "source": [
        "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
        "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
        "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
        "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
        "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUeDEAi4XgLj"
      },
      "source": [
        "<a name=\"other\"></a>\n",
        "## Classifying other labels\n",
        "\n",
        "We have been working on binary classification of gender. Other user metadata can be predicted also, with the age_range and English variety being present (if you use the larger datasets provided).\n",
        "\n",
        "Looking at age, there are 5 age ranges present, plus some marked as \"unknown\". We need to remove the \"unknowns\" as predicting as 'unknown' does not make sense. You could predict these unknowns with your trained classifier at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGP3DFaIXgLl"
      },
      "outputs": [],
      "source": [
        "set([d.meta['age_range'] for d in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PIXdY7NXgLn"
      },
      "outputs": [],
      "source": [
        "age_X = [d for d in X if d.meta['age_range'] != 'unknown']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qRE_KITXgLo"
      },
      "outputs": [],
      "source": [
        "len(age_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCvtI1hkXgLq"
      },
      "source": [
        "We need to extract the new labels also."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiMWmBH4XgLq"
      },
      "outputs": [],
      "source": [
        "age_y = [d.meta['age_range'] for d in age_X]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Aw8LQgXgLr"
      },
      "source": [
        "We do a train-test split as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqmoQ-WuXgLs"
      },
      "outputs": [],
      "source": [
        "age_X_train, age_X_test, age_y_train, age_y_test = train_test_split(age_X, age_y, test_size=0.2, random_state = 0, stratify=age_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCcEAfhcXgLs"
      },
      "source": [
        "We build a pipeline with feature union similar to earlier, except we are now also using grid search, including if to include function words or all words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTIA_9uhXgLt"
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list = [\n",
        "            ('ht', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = get_ht_fql)),\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "            ('word', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = None)), # to be set by grid search.\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "            ('stats', Pipeline([\n",
        "                ('processor', DocumentProcessor(process_method = get_text_stats)),\n",
        "                ('vectorizer', DictVectorizer()),\n",
        "            ])),\n",
        "        ],\n",
        "    )),\n",
        "    ('selector', SelectKBest(score_func = chi2)),\n",
        "    ('standardize', StandardScaler(with_mean=False)),\n",
        "    ('clf', LogisticRegression(solver='liblinear', random_state=0, multi_class='ovr')),\n",
        "])\n",
        "\n",
        "param_grid={\n",
        "    'union__word__processor__process_method': [get_fws_fql, get_tokens_fql],\n",
        "    'selector__k': [50, 100, 150, 500],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "search = GridSearchCV(model, cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
        "                      return_train_score = False, \n",
        "                      scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
        "                      refit = 'f1_weighted',\n",
        "                      param_grid = param_grid\n",
        "                     )\n",
        "\n",
        "search.fit(age_X_train, age_y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "P6euoRuzXgLu"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U52wGWeXgLv"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tksfb1FSXgLx"
      },
      "outputs": [],
      "source": [
        "predictions = search.predict(age_X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(age_y_test, predictions))\n",
        "print(classification_report(age_y_test, predictions))\n",
        "print(confusion_matrix(age_y_test, predictions))\n",
        "\n",
        "confusion_matrix_heatmap(confusion_matrix(age_y_test,predictions), search.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can save your model for later use. This is straightforward with \"pickling\", which is serialising Python objects to files. There is a nice intro to pickling here: https://medium.com/better-programming/dont-fear-the-pickle-using-pickle-dump-and-pickle-load-5212f23dbbce\n",
        "\n",
        "You can easily save a built model, e.g. the best model from a grid search, and load it for later use (e.g. making predictions)."
      ],
      "metadata": {
        "id": "eMWy0R5R6goW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZz8Sw_OXgLx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('age_model.pkl', 'wb') as f:\n",
        "    pickle.dump(search, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And load an existing model with:"
      ],
      "metadata": {
        "id": "s-ETN6Ei6uCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGWqEY0RXgLy"
      },
      "outputs": [],
      "source": [
        "with open('age_model.pkl', 'rb') as f:\n",
        "    age_model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnrLyXHzXgLy"
      },
      "outputs": [],
      "source": [
        "age_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FzmdzznXgLz"
      },
      "outputs": [],
      "source": [
        "predictions = age_model.predict(age_X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(age_y_test, predictions))\n",
        "print(classification_report(age_y_test, predictions))\n",
        "print(confusion_matrix(age_y_test, predictions))\n",
        "\n",
        "confusion_matrix_heatmap(confusion_matrix(age_y_test,predictions), search.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fB2RfAAXgLz"
      },
      "source": [
        "<a name=\"ex\"></a>\n",
        "## Exercise\n",
        "\n",
        "Try adding and improving the features used for age classification, and evaluate your results with different setups.\n",
        "\n",
        "\n",
        "### Advanced tasks:\n",
        "\n",
        "- You could try predicting the precise age with a regression model (the age is in the json metadata). See: https://dl.acm.org/citation.cfm?id=2107651.\n",
        "- Or you could try predicting age and gender together, either mutli-class pairs, or through a classification tree, e.g. predict gender first, then narrowing age ranges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yr5FbLL155xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iqZqmuTS55mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fnPEtWJD56K8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "413-wk19-classification2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}