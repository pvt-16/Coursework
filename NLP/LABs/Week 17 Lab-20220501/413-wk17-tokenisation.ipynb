{"cells":[{"cell_type":"markdown","metadata":{"id":"0Fd6Wxlg-64x"},"source":["# SCC.413 Applied Data Mining\n","# Week 17\n","# Tokenisation"]},{"cell_type":"markdown","metadata":{"id":"yRVBmY_n-644"},"source":["## Contents\n","- [Introduction](#intro)\n","- [Data](#data)\n","- [Code](#code)\n","- [Functions and imports](#functions)\n","- [Whitespace tokeniser](#whitespace)\n","- [Matching tokeniser](#matching)\n","- [NLTK tokeniser](#nltk)\n","- [Custom tokeniser](#custom)\n","- [Frequency analysis](#freq)\n","- [Exercise](#exercise)"]},{"cell_type":"markdown","metadata":{"id":"Kh7i2lmO-646"},"source":["<a name=\"intro\"></a>\n","## Introduction\n","\n","In this lab you will be honing your regular expression skills to perform the key task of tokenisation. The aim of tokenisation is to separate the text into meaningful components that are useful for future analysis (e.g. counting or annotating). Often the most logical token is a \"word\" (e.g. for \"bag-of-words\" based methods), but deciding what constitutes a word is not straight-forward. At other times, punctuation should be maintained (e.g. for part of speech annotation). You will see that choosing the wrong tokenisation can impact follow-on analysis of the text."]},{"cell_type":"markdown","metadata":{"id":"RDQ2IIYM-648"},"source":["<a name=\"data\"></a>\n","## Data\n","\n","You will be trying different tokenisers. Four texts are provided for testing the tokenisers: `tweet.txt` - a single tweet for testing, `mumsnet.txt` - a collection of mumsnet forum posts as collected in last week's lab, and `mirrormirror.txt` & `charliex.txt` - short plot summaries of Star Trek episodes from Wikipedia. You are also welcome to collect and use your own data, utilising techniques from last week's lab. `tweet.txt` will be used primarily below, but feel free to swap any text throughout to aid your understanding."]},{"cell_type":"markdown","source":["You should upload all of the provided files to a Google Drive folder, you can then access these files from your Python code. See also the files tab."],"metadata":{"id":"tS7ckSyCBCoA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"ydCluoej2UY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We save the folder we are working from as a variable for easy access. You may need to edit the path to match your own."],"metadata":{"id":"trooi5ZMzSM0"}},{"cell_type":"code","source":["working_folder = '/content/gdrive/MyDrive/413/wk17/'"],"metadata":{"id":"c1YsiHLvzKeW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The below code adds the working folder to the system path, so you can import Python files from this folder."],"metadata":{"id":"ktoDdLMkBVMX"}},{"cell_type":"code","source":["import sys\n","sys.path.append(working_folder)"],"metadata":{"id":"Cq_7TvOW34v5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEkGEVgZ-648"},"source":["<a name=\"code\"></a>\n","## Code\n","\n","If you prefer, `tokeniser.py` provides skeleton code for what is presented below. It simply runs the tokenisation method provided in `tokenise(text)` over the text file provided on the command line, printing out the tokens 1 per line (as is standard for tokenisation), and a total count of tokens found. To run the tokeniser over a file, run the below on your command line:\n","\n","```\n","$ python3 tokeniser.py infile.txt\n","```\n","\n","You can redirect the printed output to a file if you wish, simply add `> outfile.txt` to the end of the command."]},{"cell_type":"markdown","metadata":{"id":"zZa6fVAd-649"},"source":["<a name=\"functions\"></a>\n","## Functions and Imports\n","\n","We'll need regular expressions later (you might want to use the `regex` library instead). We'll also be using NLTK. You'll need to download a tokeniser model (this is only needed once)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13WiKLCs-64-"},"outputs":[],"source":["import re\n","import sys\n","import nltk"]},{"cell_type":"code","source":["nltk.download('punkt') #download tokeniser models, will download first time."],"metadata":{"id":"1Qn9dFUfNPi2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ptRNzuD-65C"},"source":["Functions will be used in this lab to define regularly used code.\n","\n","First to open a file, tokenise it with a provided tokeniser method, and return a list of tokens:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tmU1P_e-65E"},"outputs":[],"source":["# takes file and tokeniser function, reads line by line, and returns list of tokens.\n","def tokenise_file(textfile, tokenise):\n","    with open(textfile, encoding=\"utf-8\") as f:\n","        tokens = []\n","        lines = f.readlines()\n","        for line in lines:\n","            line_tokens = tokenise(line.strip())\n","            tokens.extend(line_tokens)\n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"g4SMOe5B-65E"},"source":["To print a list of tokens and the number of tokens present, and to a file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6Ji5A0y-65F"},"outputs":[],"source":["def print_tokens(tokens):\n","    for token in tokens: #iterate tokens and print one per line.\n","        print(token)\n","    print(f\"Total: {len(tokens)} tokens\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1QOddPI-65G"},"outputs":[],"source":["def save_tokens(tokens, outfile):\n","    with open(outfile, 'w', encoding=\"utf-8\") as f:\n","        for token in tokens: #iterate tokens and output to file.\n","            f.write(token + '\\n')\n","        f.write(f\"Total: {len(tokens)} tokens\")"]},{"cell_type":"markdown","metadata":{"id":"miZWiMYe-65H"},"source":["We open our tweet file as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yfLkgvo-65H"},"outputs":[],"source":["# for reference\n","with open(working_folder + \"tweet.txt\", encoding=\"utf-8\") as f:\n","    tweet = f.read()\n","print(tweet)"]},{"cell_type":"markdown","metadata":{"id":"E5Fky5BP-65I"},"source":["<a name=\"whitespace\"></a>\n","## Whitespace tokeniser"]},{"cell_type":"markdown","metadata":{"id":"EGtPn19b-65J"},"source":["The simplest method of tokenisation is to just split the text on whitespace. The tokeniser below does just this, using Python's split function, simply on a space character."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XVOqrVn-65L"},"outputs":[],"source":["def whitespace_tokenise(text):\n","    return text.split(\" \")"]},{"cell_type":"markdown","source":["We can use this tokeniser directly:"],"metadata":{"id":"jWndciZENfcF"}},{"cell_type":"code","source":["tokens = whitespace_tokenise(tweet)\n","print_tokens(tokens)"],"metadata":{"id":"jfcqhgNPNoL3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But we can also use the tokenise function as a callable object for the `tokenise_file` function:"],"metadata":{"id":"wyDfXNWUOm9m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JXCh1GC-65M"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"tweet.txt\", whitespace_tokenise)\n","print(tweet)\n","print()\n","print_tokens(tokens)"]},{"cell_type":"markdown","metadata":{"id":"ZlccySDt-65N"},"source":["Observe the output, what potential issues can you observe? Are any characters missing from the original input? Can you improve it?\n","\n","1. Can you think of a better white space tokeniser, e.g. that splits on multiple spaces, or other white space characters using a regular expression?\n","\n","2. Can you come up with an alternative than using split, and instead using findall with a regular expression to do the same? (See simple tokeniser below)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW8To9Qi-65O"},"outputs":[],"source":["# 1. answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTJmx3TN-65P"},"outputs":[],"source":["# 2. answer"]},{"cell_type":"markdown","metadata":{"id":"gcWFVr8K-65P"},"source":["<a name=\"matching\"></a>\n","## Matching tokeniser"]},{"cell_type":"markdown","metadata":{"id":"5V7aXGk1-65Q"},"source":["Next we can try a very simple tokeniser that instead of finding the space between tokens, looks for patterns that match words. We use a basic regular expression for this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIu24Jjg-65R"},"outputs":[],"source":["def simple_match_tokenise(text):\n","    p = re.compile(r\"[a-zA-Z]+\")\n","    return p.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNMsrvCa-65S"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"tweet.txt\", simple_match_tokenise)\n","print(tweet)\n","print()\n","print_tokens(tokens)"]},{"cell_type":"markdown","metadata":{"id":"7tDwzba3-65U"},"source":["Again, observe the output. What potential issues do you see? Why do you have more tokens? Are any characters missing from the original input?\n","- Can you invert the function to use split instead?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlDCvjZy-65V"},"outputs":[],"source":["# answer\n"]},{"cell_type":"markdown","metadata":{"id":"NSkB-sUa-65W"},"source":["<a name=\"nltk\"></a>\n","## NLTK Tokeniser\n","\n","All NLP toolkits, and many NLP tools themselves have built in tokenisers. [NLTK](http://www.nltk.org) is one of the best known ones and has a specialsed tokeniser for online text, and specifically Twitter data. Running the tokenisers is straight-forward, with methods provided below, `nltk_tokenise`: default, and `nltk_twitter_tokenise`: for Twitter. Note the \"Twitter\" tokeniser will be applicable to other online text too, e.g. forum data. Run these tokenisers over the texts and compare the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKYR47dj-65W"},"outputs":[],"source":["def nltk_tokenise(text):\n","    return nltk.word_tokenize(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sioNBGL-65X"},"outputs":[],"source":["def nltk_twitter_tokenise(text):\n","    twtok = nltk.tokenize.TweetTokenizer()\n","    return twtok.tokenize(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4wur250-65X"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"tweet.txt\", nltk_tokenise)\n","print(tweet)\n","print()\n","print_tokens(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ant0Jb4j-65Y"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"tweet.txt\", nltk_twitter_tokenise)\n","print(tweet)\n","print()\n","print_tokens(tokens)"]},{"cell_type":"markdown","metadata":{"id":"sfxQKkFs-65b"},"source":["What is dealt with better by NLTK? And by the Twitter tokeniser? Any potential issues outstanding?"]},{"cell_type":"markdown","metadata":{"id":"yUrozykp-65c"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"x6d7LPzZ-65d"},"source":["<a name=\"custom\"></a>\n","## Custom tokeniser"]},{"cell_type":"markdown","metadata":{"id":"q89llSiu-65f"},"source":["The main task of this lab is to write your own tokeniser to list tokens useful for different purposes and different texts. You have a template for this in `custom_tokenise`. Here, a list of regular expression patterns are used to search for different types of tokens in turn, utilising *alternation* in one large compiled regular expression. You are provided with patterns for URLs, and simple words. Try this out on the tweet, the URLs should be tokenised separately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3HHJesa-65g"},"outputs":[],"source":["def custom_tokenise(text):\n","    URL = '(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*' #this is one possible URL pattern, more complicated patterns that catch different URLs are possible.\n","    word = '\\w+'\n","    patterns = (URL, word)\n","    joint_patterns = '|'.join(patterns) #the patterns are split with | for alternation.\n","    p = re.compile(r'(?:{})'.format(joint_patterns)) # format is used to build the pattern, surrounding with (?:...) for non-captured grouping for alternation.\n","    return p.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAy7kwXQ-65j"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"tweet.txt\", custom_tokenise)\n","print(tweet)\n","print()\n","print_tokens(tokens)"]},{"cell_type":"markdown","metadata":{"id":"IoMewC2y-65k"},"source":["A point to be aware of here is the ordering of the pattern sequence. Regular expressions in Python (and most other languages) process alternation options in order (left to right). This means a longer match (i.e. greedy) may be ignored if an earlier option leads to a successful match (regular expression finds will not overlap). It is therefore important to consider the order of the regular expressions, as a more general pattern may match some text before a more specific pattern has chance to see it. You can see this in action by swapping the URL and word patterns (`patterns = (word,URL)`) and tokenising the tweet again, now the URL will not be tokenised separately as the word pattern hits first and consumes the start of the URLs."]},{"cell_type":"markdown","metadata":{"id":"SfoXeIoZ-65l"},"source":["In order to aid understanding, a tokeniser is provided below specialised for the tokenising the `mirrormirror.txt`. Here the following tokenisation rules have been applied:\n","- all punctuation is separated as individual tokens\n","- titles and initials are single tokens (e.g. `Dr.` and `T.`)\n","- hyphenated words are a single token (e.g. `mirror-universe`)\n","- cases of possessive s (`'s`) are separated, e.g. `Kirk` & `'s` are separate tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTaN5BOY-65m"},"outputs":[],"source":["def custom_tokenise_mirror(text):\n","    title = \"[A-Z][a-z]?\\.\"\n","    word = \"[-\\w]+\" # - in character set at start doesn't need escaping.\n","    apos = \"\\'[a-z]*\"\n","    other_chars = \"[^\\w\\s]\"\n","    leftover = \"\\S+\" #Having a final catch all of \"non-white-space\" will pick up anything not explicity looked for earlier.\n","    patterns = (title, apos, other_chars, word, leftover) #leftover not actually needed here, as all caught in other_chars.\n","    joint_patterns = '|'.join(patterns)\n","    p = re.compile(r'(?:{})'.format(joint_patterns))\n","    return p.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsj-qCXR-65m"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"mirrormirror.txt\", custom_tokenise_mirror)\n","save_tokens(tokens, \"mirrormirror_tok.txt\")"]},{"cell_type":"markdown","metadata":{"id":"HkxIQRcJ-65n"},"source":["Observe the output in `mirrormirror_tok.txt`. Check you understand which patterns are catching which tokens?\n","\n","Can you edit the tokeniser so that punctuation tokens aren't included, but the tokens above (titles and initials) with punctuation are still present?"]},{"cell_type":"markdown","metadata":{"id":"Iyf-bqcI-65o"},"source":["**Advanced**: Try running your tokeniser on `charliex.txt` too. This is very similar to mirrormirror.txt, but it also contains contractions (e.g. `don't`). How are these dealt with by the above tokeniser? Ideally, `n't` should be a separate token, to represent `not`. This is tricky, without breaking other parts or adding pre- or post-processing. A hint is available if you would like it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e96u54tw-65p"},"outputs":[],"source":["# Answer\n"]},{"cell_type":"markdown","metadata":{"id":"zjbPsiSC-65r"},"source":["<a name=\"freq\"></a>\n","## Frequency analysis\n","Once we have text tokenised we can start to count words, and do some analysis. We will start in earnest with this next week. For now, we will do a simple counting of tokens and producing a frequency list and plot.\n","\n","The `frequency_analysis` method takes a list of tokens (outputted from the various tokenise methods) and counts the frequency of each token. The list of tokens is printed alongside the frequency of each, in descending frequency order. A frequency plot is also produced, with the top 20 tokens as default.\n","\n","Examining the frequency list (and token streams) can help to find common tokenisation issues.\n","\n","You can try this on different texts. Increase the number of items in the plot, given enough text you should start to see a nice [Zipfian curve](https://en.wikipedia.org/wiki/Zipf\\%27s_law). Try `cumulative=True`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGM-m3jN-65s"},"outputs":[],"source":["%matplotlib inline\n","\n","def frequency_analysis(tokens, top=20):\n","    freq = nltk.FreqDist(tokens)\n","    for key,val in freq.most_common(top):\n","        print(key,val,sep=\"\\t\")\n","\n","    freq.plot(top, cumulative=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wGmAEAM-65s"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"mirrormirror.txt\", custom_tokenise_mirror)\n","frequency_analysis(tokens)"]},{"cell_type":"markdown","metadata":{"id":"KU2tFexs-65t"},"source":["<a name=\"exercise\"></a>\n","## Exercise\n","\n","Build a tokeniser for the mumsnet forum data (`mumsnet.txt`) by building on the `custom_tokenise` method. Produce the frequency analysis as above. Tokenise the text with the following rules:\n","- all punctuation separated as individual tokens, unless sequences of punctuation (e.g. `!!!`), which should be combined to a single token.\n","- URLs, hashtags, and mentions as separate tokens\n","- hyphenated words as a single token\n","- words with apostrophes should be a single token (e.g. `don't`, `I'm` and `1940's`)\n","- **Advanced/Extra**: Emoticons separated as separate tokens (e.g. `:-)`)\n","- **Advanced/Extra**: You will find other sequences in the text that should be single tokens, deal with as many of these as possible for as clean a tokenisation as possible ready for creating a frequency list.\n","\n","You may want to consider pre-processing (i.e. cleaning/normalising) the text to make tokenisation easier.\n","\n","Remember, it's important to look at the text, the token list, and the frequency analysis to see issues you may need to deal with."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQE3Bsma-65u"},"outputs":[],"source":["# Exercise, edit the function below\n","\n","def custom_tokenise_forum(text):\n","    URL = '(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*' #this is one possible URL pattern, more complicated patterns that catch different URLs are possible.\n","    word = '\\w+'\n","    patterns = (URL, word)\n","    joint_patterns = '|'.join(patterns) #the patterns are split with | for alternation.\n","    p = re.compile(r'(?:{})'.format(joint_patterns)) # format is used to build the pattern, surrounding with (?:...) for non-captured grouping for alternation.\n","    return p.findall(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxELknxS-65w"},"outputs":[],"source":["tokens = tokenise_file(working_folder + \"mumsnet.txt\", custom_tokenise_forum)\n","save_tokens(tokens,\"mumsnet_tok.txt\")\n","frequency_analysis(tokens,100)"]},{"cell_type":"markdown","metadata":{"id":"6e-DJMvW-65w"},"source":["Note any findings or decisions made here:\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"name":"413-wk17-tokenisation.ipynb","provenance":[{"file_id":"1uOafr2aoe96wYKdYut1b5UaHn5VJr9lc","timestamp":1646165519594}],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}